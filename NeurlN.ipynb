{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T11:56:10.217498Z",
     "start_time": "2021-04-28T11:56:10.213498Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T11:56:10.230508Z",
     "start_time": "2021-04-28T11:56:10.220500Z"
    }
   },
   "outputs": [],
   "source": [
    "#Ham sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T11:56:10.238513Z",
     "start_time": "2021-04-28T11:56:10.234510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dao ham sigmoid: Kỹ thuật này sử dụng giảm độ dốc để tìm một tập hợp các tham số mô hình tối ưu để giảm thiểu hàm mất. \n",
    "#Trong ví dụ của bạn, bạn phải sử dụng đạo hàm của một sigmoid bởi vì đó là kích hoạt mà các nơ-ron riêng lẻ của bạn đang sử dụng.\n",
    "def sigmoid_derivative(x):\n",
    "    return x*(1-x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:02:46.198712Z",
     "start_time": "2021-04-28T12:02:46.183071Z"
    }
   },
   "outputs": [],
   "source": [
    "#Lop neuralNetword\n",
    "class NeuralNetword:\n",
    "    def __init__(self,layers,alpha=0.1):\n",
    "        # Mô h…nh layer v‰ dụ [2,2,1]\n",
    "        self.layers=layers\n",
    "        #he so learning rate\n",
    "        self.alpha=alpha\n",
    "        #tham so w,b\n",
    "        self.W=[]\n",
    "        self.b=[]\n",
    "        #khoi tao cac tham so o moi layer\n",
    "        for i in range (0, len(layers)-1):\n",
    "            w_=np.random.randn(layers[i],layers[i+1])\n",
    "            b_=np.zeros((layers[i+1],1))\n",
    "            self.W.append(w_/layers[i])\n",
    "            self.b.append(b_)\n",
    "    #tom tat mo hinh neuralNetword\n",
    "    def __repr__(self):\n",
    "        return 'NeuralNetword[{}]'.format('-'.join(str(l)for l in self.layers))\n",
    "#train mo hinh voi du lieu\n",
    "    def fit_partial(self,x,y):\n",
    "        A=[x]\n",
    "        #qua trinh feedforward(lan truyen tien)\n",
    "        out=A[-1]\n",
    "        for i in range(0,len (self.layers)-1):\n",
    "            out = sigmoid(np.dot(out,self.W[i])+(self.b[i].T))\n",
    "            A.append(out)\n",
    "            \n",
    "        #qua trinh backpropagation(lan truyen nguoc)\n",
    "        y=y.reshape(-1,1)\n",
    "        dA=[-(y/A[-1] - (1-y)/(1-A[-1]))]\n",
    "        dW=[]\n",
    "        db=[]\n",
    "        for i in reversed(range(0,len(self.layers)-1)):\n",
    "            dw_=np.dot((A[i]).T,dA[-1]*sigmoid_derivative(A[i+1]))\n",
    "            db_=(np.sum(dA[-1]*sigmoid_derivative(A[i+1]),0)).reshape(-1,1)\n",
    "            dA_=np.dot(dA[-1]*sigmoid_derivative(A[i+1]),self.W[i].T)\n",
    "            dW.append(dw_)\n",
    "            db.append(db_)\n",
    "            dA.append(dA_)\n",
    "        #Dao nguoc dW,db\n",
    "        dW=dW[::-1]\n",
    "        db=db[::-1]\n",
    "        #Gradient descent\n",
    "        for i in range (0,len(self.layers)-1):\n",
    "            self.W[i]=self.W[i]-self.alpha*dW[i]\n",
    "            self.b[i]=self.b[i]-self.alpha*db[i]\n",
    "            \n",
    "    def fit (self, X, y, epochs=20, verbose=10):\n",
    "        for epochs in range (0,epochs):\n",
    "            self.fit_partial(X,y)\n",
    "            if epochs % verbose ==0 :\n",
    "                loss=self.calculate_loss(X,y)\n",
    "                print('Epochs{},loss{}'.format(epochs,loss))\n",
    "    #Du doan\n",
    "    def predict (self,X):\n",
    "        for i in range(0,len(self.layers)-1):\n",
    "            X=sigmoid(np.dot(X , self.W[i]) + (self.b[i].T))\n",
    "            \n",
    "        return X\n",
    "\n",
    "    # Tinh ham loss function\n",
    "    def calculate_loss (self,X,y):\n",
    "        y_predict=self.predict(X)\n",
    "        return -(np.sum(y*np.log(y_predict)+(1-y)*np.log(1-y_predict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:10:42.676245Z",
     "start_time": "2021-04-28T12:10:42.649222Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[10.    1.    1.  ]\n",
      " [ 5.    2.    1.  ]\n",
      " [ 6.    1.8   1.  ]\n",
      " [ 7.    1.    1.  ]\n",
      " [ 8.    2.    1.  ]\n",
      " [ 9.    0.5   1.  ]\n",
      " [ 4.    3.    1.  ]\n",
      " [ 5.    2.5   1.  ]\n",
      " [ 8.    1.    1.  ]\n",
      " [ 4.    2.5   1.  ]\n",
      " [ 8.    0.1   0.  ]\n",
      " [ 7.    0.15  0.  ]\n",
      " [ 4.    1.    0.  ]\n",
      " [ 5.    0.8   0.  ]\n",
      " [ 7.    0.3   0.  ]\n",
      " [ 4.    1.    0.  ]\n",
      " [ 5.    0.5   0.  ]\n",
      " [ 6.    0.3   0.  ]\n",
      " [ 7.    0.2   0.  ]\n",
      " [ 8.    0.15  0.  ]]\n",
      "[[10.    1.  ]\n",
      " [ 5.    2.  ]\n",
      " [ 6.    1.8 ]\n",
      " [ 7.    1.  ]\n",
      " [ 8.    2.  ]\n",
      " [ 9.    0.5 ]\n",
      " [ 4.    3.  ]\n",
      " [ 5.    2.5 ]\n",
      " [ 8.    1.  ]\n",
      " [ 4.    2.5 ]\n",
      " [ 8.    0.1 ]\n",
      " [ 7.    0.15]\n",
      " [ 4.    1.  ]\n",
      " [ 5.    0.8 ]\n",
      " [ 7.    0.3 ]\n",
      " [ 4.    1.  ]\n",
      " [ 5.    0.5 ]\n",
      " [ 6.    0.3 ]\n",
      " [ 7.    0.2 ]\n",
      " [ 8.    0.15]]\n",
      "Epochs0,loss14.040574170939985\n",
      "Epochs100,loss8.672229007408854\n",
      "Epochs200,loss13.07928029053924\n",
      "Epochs300,loss8.273987964026308\n",
      "Epochs400,loss10.857808754646655\n",
      "Epochs500,loss5.781528817690205\n",
      "Epochs600,loss10.965957786757949\n",
      "Epochs700,loss7.071737125592707\n",
      "Epochs800,loss9.586265867467294\n",
      "Epochs900,loss0.23660154268029746\n",
      "Epochs1000,loss0.12282517094492967\n",
      "Epochs1100,loss0.08270843009736605\n",
      "Epochs1200,loss0.06233099298424304\n",
      "Epochs1300,loss0.050019624584208645\n",
      "Epochs1400,loss0.04178015650092016\n",
      "Epochs1500,loss0.03587969710518804\n",
      "Epochs1600,loss0.03144588616450168\n",
      "Epochs1700,loss0.027992034081856885\n",
      "Epochs1800,loss0.025225300551615808\n",
      "Epochs1900,loss0.022958954638588935\n",
      "Epochs2000,loss0.02106831767138914\n",
      "Epochs2100,loss0.019466968127554683\n",
      "Epochs2200,loss0.01809311382542824\n",
      "Epochs2300,loss0.01690140113281222\n",
      "Epochs2400,loss0.015857788743956252\n",
      "Epochs2500,loss0.014936227039895077\n",
      "Epochs2600,loss0.014116442288371984\n",
      "Epochs2700,loss0.013382419379932575\n",
      "Epochs2800,loss0.012721339018145612\n",
      "Epochs2900,loss0.012122818110627888\n",
      "Epochs3000,loss0.011578357025151262\n",
      "Epochs3100,loss0.011080930836352526\n",
      "Epochs3200,loss0.0106246826193961\n",
      "Epochs3300,loss0.010204690252552991\n",
      "Epochs3400,loss0.009816786961404747\n",
      "Epochs3500,loss0.009457421687900524\n",
      "Epochs3600,loss0.00912354933945835\n",
      "Epochs3700,loss0.00881254371389662\n",
      "Epochs3800,loss0.008522127815142071\n",
      "Epochs3900,loss0.008250317637203467\n",
      "Epochs4000,loss0.00799537647358244\n",
      "Epochs4100,loss0.007755777521983908\n",
      "Epochs4200,loss0.0075301730784569495\n",
      "Epochs4300,loss0.007317369004597164\n",
      "Epochs4400,loss0.007116303443714456\n",
      "Epochs4500,loss0.006926028983063255\n",
      "Epochs4600,loss0.0067456976280993305\n",
      "Epochs4700,loss0.006574548084671764\n",
      "Epochs4800,loss0.006411894945755101\n",
      "Epochs4900,loss0.006257119457976756\n",
      "Epochs5000,loss0.006109661604979231\n",
      "Epochs5100,loss0.005969013293521561\n",
      "Epochs5200,loss0.005834712467131298\n",
      "Epochs5300,loss0.005706338003203892\n",
      "Epochs5400,loss0.005583505274504665\n",
      "Epochs5500,loss0.0054658622762448405\n",
      "Epochs5600,loss0.0053530862363780005\n",
      "Epochs5700,loss0.005244880640188114\n",
      "Epochs5800,loss0.005140972611238945\n",
      "Epochs5900,loss0.005041110599858102\n",
      "Epochs6000,loss0.00494506233781431\n",
      "Epochs6100,loss0.0048526130240798306\n",
      "Epochs6200,loss0.004763563711779087\n",
      "Epochs6300,loss0.00467772987074158\n",
      "Epochs6400,loss0.004594940103763458\n",
      "Epochs6500,loss0.004515034997713157\n",
      "Epochs6600,loss0.004437866093243109\n",
      "Epochs6700,loss0.004363294959057209\n",
      "Epochs6800,loss0.004291192358557037\n",
      "Epochs6900,loss0.00422143749827458\n",
      "Epochs7000,loss0.0041539173488905935\n",
      "Epochs7100,loss0.004088526030768397\n",
      "Epochs7200,loss0.004025164256970902\n",
      "Epochs7300,loss0.0039637388275803225\n",
      "Epochs7400,loss0.0039041621698831072\n",
      "Epochs7500,loss0.00384635191963863\n",
      "Epochs7600,loss0.0037902305392167575\n",
      "Epochs7700,loss0.0037357249688612757\n",
      "Epochs7800,loss0.0036827663077765886\n",
      "Epochs7900,loss0.00363128952210308\n",
      "Epochs8000,loss0.003581233177176655\n",
      "Epochs8100,loss0.0035325391917329528\n",
      "Epochs8200,loss0.003485152612002991\n",
      "Epochs8300,loss0.0034390214038365744\n",
      "Epochs8400,loss0.003394096261192906\n",
      "Epochs8500,loss0.0033503304295288442\n",
      "Epochs8600,loss0.0033076795427282755\n",
      "Epochs8700,loss0.0032661014723953936\n",
      "Epochs8800,loss0.0032255561884229713\n",
      "Epochs8900,loss0.003186005629856526\n",
      "Epochs9000,loss0.0031474135851959645\n",
      "Epochs9100,loss0.0031097455813176673\n",
      "Epochs9200,loss0.003072968780321321\n",
      "Epochs9300,loss0.003037051883629937\n",
      "Epochs9400,loss0.003001965042774409\n",
      "Epochs9500,loss0.0029676797763081918\n",
      "Epochs9600,loss0.002934168892376585\n",
      "Epochs9700,loss0.0029014064164876795\n",
      "Epochs9800,loss0.0028693675240971266\n",
      "Epochs9900,loss0.0028380284776154856\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('dataset.csv').values\n",
    "print(data)\n",
    "N,d=data.shape\n",
    "X = data[:, 0:d-1].reshape(-1, d-1)\n",
    "print(X)\n",
    "#.reshape(-1, d-1)\n",
    "y = data[:, 2].reshape(-1, 1)\n",
    "\n",
    "p = NeuralNetword([X.shape[1], 2, 1], 0.1)\n",
    "p.fit(X, y, 10000, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ket qua du doan: [[0.99993492]]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([[10,1]])\n",
    "print('Ket qua du doan:',p.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}